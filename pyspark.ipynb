{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Application of Apache Spark</h1>\n",
    "<h3>with Feature Engineering</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Focus on feature selection, processing speed, model iteration</h4>\n",
    "<ul>\n",
    "    <li>Will create different training sets w/ reature mixes</li>\n",
    "    <li>PyTorch, Numba, Parralization, and Dask for processing speed</li>\n",
    "    <li>Processing tutorials: \n",
    "        <a href = 'https://towardsdatascience.com/speed-up-your-algorithms-part-3-parallelization-4d95c0888748'>speeding up your algorithms</a>, \n",
    "        <a href = 'https://towardsdatascience.com/improving-random-forest-in-python-part-1-893916666cd'>improving random forest</a>\n",
    "    </li>\n",
    "    <li>Split train into 3 train/test splits, run models and compare results before running on final test</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload #for changes in helpers\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import helpers\n",
    "reload(helpers)\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../geotab-data/train.csv')\n",
    "test = pd.read_csv('../geotab-data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Target PCA</h1>\n",
    "\n",
    "<ul>\n",
    "    <li>Light summary eda</li>\n",
    "    <li>Min max scaler and target PCA</li>\n",
    "    <li>Not sure if this is necessary for purpose of this notebook</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Summaries:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dtypes</th>\n",
       "      <th>missing</th>\n",
       "      <th>unique</th>\n",
       "      <th>first_val</th>\n",
       "      <th>last_val</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stdev</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>TotalTimeStopped_p20</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>1.731272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.080017</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>TotalTimeStopped_p50</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>7.681874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.553418</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>TotalTimeStopped_p80</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>22.948071</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.118134</td>\n",
       "      <td>5.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DistanceToFirstStop_p20</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>3479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1902.7</td>\n",
       "      <td>6.564450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.003261</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>DistanceToFirstStop_p50</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>7483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3099.5</td>\n",
       "      <td>28.255852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.720090</td>\n",
       "      <td>4.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>DistanceToFirstStop_p80</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>13267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4064.3</td>\n",
       "      <td>81.922639</td>\n",
       "      <td>60.4</td>\n",
       "      <td>152.682760</td>\n",
       "      <td>8.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name   dtypes  missing  unique  first_val  last_val     max       mean  median       stdev  entropy\n",
       "0     TotalTimeStopped_p20    int64        0     172        0.0       0.0   273.0   1.731272     0.0    7.080017     0.92\n",
       "1     TotalTimeStopped_p50    int64        0     264        0.0       0.0   343.0   7.681874     0.0   15.553418     2.70\n",
       "2     TotalTimeStopped_p80    int64        0     403        0.0       0.0   689.0  22.948071    16.0   28.118134     5.06\n",
       "3  DistanceToFirstStop_p20  float64        0    3479        0.0       0.0  1902.7   6.564450     0.0   28.003261     1.35\n",
       "4  DistanceToFirstStop_p50  float64        0    7483        0.0       0.0  3099.5  28.255852     0.0   71.720090     4.16\n",
       "5  DistanceToFirstStop_p80  float64        0   13267        0.0       0.0  4064.3  81.922639    60.4  152.682760     8.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets = train.iloc[:,12:27]\n",
    "\n",
    "#targets to predict\n",
    "total_time = targets[['TotalTimeStopped_p20','TotalTimeStopped_p50', 'TotalTimeStopped_p80']]\n",
    "distance_to_first = targets[['DistanceToFirstStop_p20','DistanceToFirstStop_p50','DistanceToFirstStop_p80']]\n",
    "target_cols = list(total_time.columns) + list(distance_to_first.columns)\n",
    "p_targets = targets[target_cols]\n",
    "\n",
    "#optional targets\n",
    "time_from_first = targets[['TimeFromFirstStop_p20','TimeFromFirstStop_p50','TimeFromFirstStop_p80']]\n",
    "\n",
    "print('Target Summaries:')\n",
    "display(helpers.summarize(p_targets, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66396904, 0.17536384, 0.07856878])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scale targets w/ a min max scalers and append to train\n",
    "for col in target_cols:\n",
    "    train[col+str(\"_minmax\")] = (preprocessing.minmax_scale(train[col], feature_range=(0,1)))\n",
    "    \n",
    "min_max_cols = ['TotalTimeStopped_p20_minmax', 'TotalTimeStopped_p50_minmax',\n",
    "                'TotalTimeStopped_p80_minmax', 'DistanceToFirstStop_p20_minmax',\n",
    "                'DistanceToFirstStop_p50_minmax', 'DistanceToFirstStop_p80_minmax']\n",
    "\n",
    "pca = PCA(n_components=3, random_state=5)\n",
    "\n",
    "principalComponents = pca.fit_transform(train[min_max_cols])\n",
    "principalDf = pd.DataFrame(principalComponents)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time and day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making hour columns cyclical\n",
    "train = helpers.date_cyc_enc(train, 'Hour', 24)\n",
    "test = helpers.date_cyc_enc(test, 'Hour', 24) \n",
    "\n",
    "#encoding time of day\n",
    "train['is_day'] = train['Hour'].apply(lambda x: 1 if 7 < x < 18 else 0)\n",
    "test['is_day'] = test['Hour'].apply(lambda x: 1 if 7 < x < 18 else 0)\n",
    "\n",
    "train['is_morning'] = train['Hour'].apply(lambda x: 1 if 6 < x < 10 else 0)\n",
    "test['is_morning'] = test['Hour'].apply(lambda x: 1 if 6 < x < 10 else 0)\n",
    "\n",
    "train['is_night'] = train['Hour'].apply(lambda x: 1 if 17 < x < 20 else 0)\n",
    "test['is_night'] = test['Hour'].apply(lambda x: 1 if 17 < x < 20 else 0)\n",
    "\n",
    "#encoding weekend vs not weekend\n",
    "train['is_day_weekend'] = np.where((train['is_day'] == 1) & (train['Weekend'] == 1), 1,0)\n",
    "test['is_day_weekend'] = np.where((test['is_day'] == 1) & (train['Weekend'] == 1), 1,0)\n",
    "\n",
    "train['is_mor_weekend'] = np.where((train['is_morning'] == 1) & (train['Weekend'] == 1), 1,0)\n",
    "test['is_mor_weekend'] = np.where((test['is_morning'] == 1) & (train['Weekend'] == 1), 1,0)\n",
    "\n",
    "train['is_nig_weekend'] = np.where((train['is_night'] == 1) & (train['Weekend'] == 1), 1,0)\n",
    "test['is_nig_weekend'] = np.where((test['is_night'] == 1) & (train['Weekend'] == 1), 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location and direction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intersection and city concat\n",
    "train[\"Intersec\"] = train[\"IntersectionId\"].astype(str) + train[\"City\"]\n",
    "test[\"Intersec\"] = test[\"IntersectionId\"].astype(str) + test[\"City\"]\n",
    "\n",
    "#make numerical and drop concat\n",
    "le = LabelEncoder()\n",
    "le.fit(pd.concat([train[\"Intersec\"],test[\"Intersec\"]]).drop_duplicates().values)\n",
    "train[\"Intersec\"] = le.transform(train[\"Intersec\"])\n",
    "test[\"Intersec\"] = le.transform(test[\"Intersec\"])\n",
    "\n",
    "train['EntryType'] = train['EntryStreetName'].apply(helpers.road_encode)\n",
    "train['ExitType'] = train['ExitStreetName'].apply(helpers.road_encode)\n",
    "test['EntryType'] = test['EntryStreetName'].apply(helpers.road_encode)\n",
    "test['ExitType'] = test['ExitStreetName'].apply(helpers.road_encode)\n",
    "\n",
    "#map directional encoding in in train and test\n",
    "train['EntryHeading'] = train['EntryHeading'].map(helpers.directions)\n",
    "train['ExitHeading'] = train['ExitHeading'].map(helpers.directions)\n",
    "test['EntryHeading'] = test['EntryHeading'].map(helpers.directions)\n",
    "test['ExitHeading'] = test['ExitHeading'].map(helpers.directions)\n",
    "\n",
    "#heading differences will tell us the degree at which you traveled, (0 degrees is straight, 180 u turn)\n",
    "train['diffHeading'] = train['EntryHeading']-train['ExitHeading']  \n",
    "test['diffHeading'] = test['EntryHeading']-test['ExitHeading'] \n",
    "\n",
    "#if you stay on the same street feature\n",
    "train[\"same_str\"] = (train[\"EntryStreetName\"] ==  train[\"ExitStreetName\"]).astype(int)\n",
    "test[\"same_str\"] = (test[\"EntryStreetName\"] ==  test[\"ExitStreetName\"]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondary features, monthly rainfall by city "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the city and month into one variable\n",
    "train['city_month'] = train[\"City\"] + train[\"Month\"].astype(str)\n",
    "test['city_month'] = test[\"City\"] + test[\"Month\"].astype(str)\n",
    "\n",
    "# Creating a new column by mapping the city_month variable to it's corresponding average monthly rainfall\n",
    "train[\"average_rainfall\"] = train['city_month'].map(helpers.monthly_rainfall)\n",
    "test[\"average_rainfall\"] = test['city_month'].map(helpers.monthly_rainfall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace city w/ dummy variables, can't run cells above after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy variables for city, drops city\n",
    "try:\n",
    "    train = pd.get_dummies(train, columns=['City' ],prefix=['City'], drop_first=False)\n",
    "    test = pd.get_dummies(test, columns=['City' ],prefix=['City'], drop_first=False)\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale lat and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wonder if this would be different with min max\n",
    "scaler = preprocessing.StandardScaler()\n",
    "for col in ['Latitude','Longitude']:\n",
    "    scaler.fit(train[col].values.reshape(-1, 1))\n",
    "    train[col] = scaler.transform(train[col].values.reshape(-1, 1))\n",
    "    test[col] = scaler.transform(test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['RowId', 'Path','EntryStreetName','ExitStreetName'],axis=1, inplace=True)\n",
    "test.drop(['RowId', 'Path','EntryStreetName','ExitStreetName'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = ['IntersectionId', 'Latitude', 'Longitude', 'EntryHeading',\n",
    "                    'ExitHeading', 'Hour', 'Weekend', 'Month',\n",
    "                    'is_morning', 'is_night', 'is_day_weekend', 'is_mor_weekend',\n",
    "                    'is_nig_weekend', \n",
    "                    #'Hour', \n",
    "                    'Hour_sin', \n",
    "                    'Hour_cos', \n",
    "                    'same_str', 'Intersec', 'EntryType',\n",
    "                    'ExitType', 'diffHeading', 'average_rainfall', 'is_day',\n",
    "                    'City_Boston', 'City_Chicago', 'City_Philadelphia', \n",
    "                    'City_Atlanta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>PySpark</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, col\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.mllib.evaluation import RegressionMetrics\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "\n",
    "from matplotlib import rcParams\n",
    "sns.set(context='notebook', style='whitegrid', rc={'figure.figsize': (18,4)})\n",
    "rcParams['figure.figsize'] = 18,4\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emag3\\Documents\\Code\\Spark\\spark-2.4.4-bin-hadoop2.7\n",
      "C:\\Users\\emag3\\Java\n",
      "C:\\Users\\emag3\\Documents\\Code\\Spark\\hadoop\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "print(os.environ['SPARK_HOME'])\n",
    "print(os.environ['JAVA_HOME'])\n",
    "print(os.environ['HADOOP_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "  spark = SparkSession.builder.appName(\"Intersection\").getOrCreate()\n",
    "  sc = spark.sparkContext\n",
    "  return spark,sc\n",
    "\n",
    "def main():\n",
    "  spark,sc = init_spark()\n",
    "  nums = sc.parallelize([1,2,3,4])\n",
    "  print(nums.map(lambda x: x*x).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>testing function</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<pyspark.sql.session.SparkSession at 0x124de7677c8>,\n",
       " <SparkContext master=local[*] appName=Intersection>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "init_spark()\n",
    "main()\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Intersection\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>convert pandas dataframe to Apache Spark dataframe</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "sp_train = sqlContext.createDataFrame(train)\n",
    "sp_test = sqlContext.createDataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+------------------+------------+-----------+----+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+-----------------------+-----------------------+-----------------------+-----------------------+-----------------------+---------------------------+---------------------------+---------------------------+------------------------------+------------------------------+------------------------------+-------------------+------------------+------+----------+--------+--------------+--------------+--------------+--------+---------+--------+-----------+--------+----------+----------------+------------+-----------+------------+-----------------+\n",
      "|IntersectionId|           Latitude|         Longitude|EntryHeading|ExitHeading|Hour|Weekend|Month|TotalTimeStopped_p20|TotalTimeStopped_p40|TotalTimeStopped_p50|TotalTimeStopped_p60|TotalTimeStopped_p80|TimeFromFirstStop_p20|TimeFromFirstStop_p40|TimeFromFirstStop_p50|TimeFromFirstStop_p60|TimeFromFirstStop_p80|DistanceToFirstStop_p20|DistanceToFirstStop_p40|DistanceToFirstStop_p50|DistanceToFirstStop_p60|DistanceToFirstStop_p80|TotalTimeStopped_p20_minmax|TotalTimeStopped_p50_minmax|TotalTimeStopped_p80_minmax|DistanceToFirstStop_p20_minmax|DistanceToFirstStop_p50_minmax|DistanceToFirstStop_p80_minmax|           Hour_sin|          Hour_cos|is_day|is_morning|is_night|is_day_weekend|is_mor_weekend|is_nig_weekend|Intersec|EntryType|ExitType|diffHeading|same_str|city_month|average_rainfall|City_Atlanta|City_Boston|City_Chicago|City_Philadelphia|\n",
      "+--------------+-------------------+------------------+------------+-----------+----+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+-----------------------+-----------------------+-----------------------+-----------------------+-----------------------+---------------------------+---------------------------+---------------------------+------------------------------+------------------------------+------------------------------+-------------------+------------------+------+----------+--------+--------------+--------------+--------------+--------+---------+--------+-----------+--------+----------+----------------+------------+-----------+------------+-----------------+\n",
      "|             0|-2.0083470719728793|-1.092677581837767|        1.75|       1.75|   0|      0|    6|                   0|                   0|                   0|                   0|                   0|                    0|                    0|                    0|                    0|                    0|                    0.0|                    0.0|                    0.0|                    0.0|                    0.0|                        0.0|                        0.0|                        0.0|                           0.0|                           0.0|                           0.0|                0.0|               1.0|     0|         0|       0|             0|             0|             0|       0|        4|       4|        0.0|       1|  Atlanta6|            3.63|           1|          0|           0|                0|\n",
      "|             0|-2.0083470719728793|-1.092677581837767|        0.75|       0.75|   0|      0|    6|                   0|                   0|                   0|                   0|                   0|                    0|                    0|                    0|                    0|                    0|                    0.0|                    0.0|                    0.0|                    0.0|                    0.0|                        0.0|                        0.0|                        0.0|                           0.0|                           0.0|                           0.0|                0.0|               1.0|     0|         0|       0|             0|             0|             0|       0|        4|       4|        0.0|       1|  Atlanta6|            3.63|           1|          0|           0|                0|\n",
      "|             0|-2.0083470719728793|-1.092677581837767|        1.75|       1.75|   1|      0|    6|                   0|                   0|                   0|                   0|                   0|                    0|                    0|                    0|                    0|                    0|                    0.0|                    0.0|                    0.0|                    0.0|                    0.0|                        0.0|                        0.0|                        0.0|                           0.0|                           0.0|                           0.0|0.25881904510252074|0.9659258262890683|     0|         0|       0|             0|             0|             0|       0|        4|       4|        0.0|       1|  Atlanta6|            3.63|           1|          0|           0|                0|\n",
      "+--------------+-------------------+------------------+------------+-----------+----+-------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+---------------------+---------------------+---------------------+---------------------+---------------------+-----------------------+-----------------------+-----------------------+-----------------------+-----------------------+---------------------------+---------------------------+---------------------------+------------------------------+------------------------------+------------------------------+-------------------+------------------+------+----------+--------+--------------+--------------+--------------+--------+---------+--------+-----------+--------+----------+----------------+------------+-----------+------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_train.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+-------------------+------------+-----------+----+-------+-----+-------------------+------------------+------+----------+--------+--------------+--------------+--------------+--------+---------+--------+-----------+--------+----------+----------------+------------+-----------+------------+-----------------+\n",
      "|IntersectionId|           Latitude|          Longitude|EntryHeading|ExitHeading|Hour|Weekend|Month|           Hour_sin|          Hour_cos|is_day|is_morning|is_night|is_day_weekend|is_mor_weekend|is_nig_weekend|Intersec|EntryType|ExitType|diffHeading|same_str|city_month|average_rainfall|City_Atlanta|City_Boston|City_Chicago|City_Philadelphia|\n",
      "+--------------+-------------------+-------------------+------------+-----------+----+-------+-----+-------------------+------------------+------+----------+--------+--------------+--------------+--------------+--------+---------+--------+-----------+--------+----------+----------------+------------+-----------+------------+-----------------+\n",
      "|             1|-2.0222886593121725|-1.0864859533442746|        1.25|       0.75|   0|      0|    6|                0.0|               1.0|     0|         0|       0|             0|             0|             0|    2602|        2|       2|        0.5|       0|  Atlanta6|            3.63|           1|          0|           0|                0|\n",
      "|             1|-2.0222886593121725|-1.0864859533442746|        1.25|       1.25|   0|      0|    6|                0.0|               1.0|     0|         0|       0|             0|             0|             0|    2602|        2|       2|        0.0|       1|  Atlanta6|            3.63|           1|          0|           0|                0|\n",
      "|             1|-2.0222886593121725|-1.0864859533442746|        0.25|       0.25|   1|      0|    6|0.25881904510252074|0.9659258262890683|     0|         0|       0|             0|             0|             0|    2602|        2|       2|        0.0|       1|  Atlanta6|            3.63|           1|          0|           0|                0|\n",
      "+--------------+-------------------+-------------------+------------+-----------+----+-------+-----+-------------------+------------------+------+----------+--------+--------------+--------------+--------------+--------+---------+--------+-----------+--------+----------+----------------+------------+-----------+------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sp_test.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "857409"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model building</h1>\n",
    "Useful Vaiables:\n",
    "<ul>\n",
    "    <li>final_features - list final set of features for prediction</li>\n",
    "    <li>target_cols - list of targets to predict</li>\n",
    "    <li>train - full train data set with derived features</li>\n",
    "    <li>test - full test data set with derived features</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (857409, 48)\n",
      "Test dataset shape:  (1920335, 27)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset shape: \"+ str(train.shape))\n",
    "print(\"Test dataset shape:  \"+ str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emag3\\Documents\\Code\\Python\\geotab-intersection\\geotab-intersection\\helpers.py:111: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(np.int16)\n",
      "C:\\Users\\emag3\\Documents\\Code\\Python\\geotab-intersection\\geotab-intersection\\helpers.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(np.float64)\n",
      "C:\\Users\\emag3\\Documents\\Code\\Python\\geotab-intersection\\geotab-intersection\\helpers.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[col] = df[col].astype(np.int8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 68.69 Mb (47.5% reduction)\n",
      "Mem. usage decreased to 153.84 Mb (47.5% reduction)\n"
     ]
    }
   ],
   "source": [
    "#X and y for train set\n",
    "X = train[final_features]\n",
    "y = train[target_cols]\n",
    "ys = [train[column] for column in target_cols]\n",
    "\n",
    "#only run this at the end, to get final prediction\n",
    "X_final = test[final_features]\n",
    "\n",
    "#reduce mem usage on feature sets (X)\n",
    "X = helpers.reduce_mem_usage(X)\n",
    "X_final = helpers.reduce_mem_usage(X_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Model iteration and validation</h2>\n",
    "<ul>\n",
    "    <li>Below we split original train into 2 train/tests using different random seeds</li>\n",
    "    <li>We do so to test accuracy of model before crating final sumbission, which we can't pre score</li>\n",
    "    <li>First item is to test model code, uses random 10k sample to make sure code runs before we leave i\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.25\n",
    "#75% to train and 25% to test\n",
    "\n",
    "#this is just for to ensure running models works head(10k), speed cuz 10k is small and it should run fast\n",
    "SPEED = train.sample(10000,random_state=2)\n",
    "X_SPEED = SPEED[final_features] \n",
    "Y_SPEED = SPEED[target_cols]\n",
    "\n",
    "X_train_SPEED, X_validation_SPEED, Y_train_SPEED, Y_validation_SPEED = train_test_split(X_SPEED, Y_SPEED, test_size=validation_size, random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Real Validators</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_1, X_validation_1, Y_train_1, Y_validation_1 = train_test_split(X, y, test_size=validation_size, random_state=7)\n",
    "X_train_2, X_validation_2, Y_train_2, Y_validation_2 = train_test_split(X, y, test_size=validation_size, random_state=23232)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sets is a list of X_train_1, X_validation_1, Y_train_1, Y_validation_1 for every target value\n",
    "sets = []\n",
    "for y_target in ys:\n",
    "    sets.append(train_test_split(X, y_target, test_size=validation_size, random_state=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']\n"
     ]
    }
   ],
   "source": [
    "print(target_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "DistanceToFirstStop_p80 = sets[-1]\n",
    "\n",
    "clf = AdaBoostRegressor(n_estimators=60)\n",
    "clf_results = helpers.run_model(clf,DistanceToFirstStop_p80[0], DistanceToFirstStop_p80[1], DistanceToFirstStop_p80[2], DistanceToFirstStop_p80[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_time': '0:01:02.673537', 'run_score': 172.2698934659224}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#score: 215.33 > 193.5 > 188.39\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_time': '0:00:20.635369', 'run_score': 115.42958679796664}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#n=5, score: 114.8\n",
    "neigh = KNeighborsRegressor(n_neighbors=5)\n",
    "clf_results = helpers.run_model(neigh,DistanceToFirstStop_p80[0], DistanceToFirstStop_p80[1], DistanceToFirstStop_p80[2], DistanceToFirstStop_p80[3])\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_time': '0:00:00.987725', 'run_score': 150.92582321936717}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression()\n",
    "clf_results = helpers.run_model(reg,DistanceToFirstStop_p80[0], DistanceToFirstStop_p80[1], DistanceToFirstStop_p80[2], DistanceToFirstStop_p80[3])\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Ridge' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-16f75c47d436>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRidge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.005\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mclf_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDistanceToFirstStop_p80\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDistanceToFirstStop_p80\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDistanceToFirstStop_p80\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDistanceToFirstStop_p80\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclf_results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Ridge' is not defined"
     ]
    }
   ],
   "source": [
    "#reg = Ridge(alpha=0.005)\n",
    "#clf_results = helpers.run_model(reg,DistanceToFirstStop_p80[0], DistanceToFirstStop_p80[1], DistanceToFirstStop_p80[2], DistanceToFirstStop_p80[3])\n",
    "#clf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_time': '0:00:02.160881', 'run_score': 150.92574848028391}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = BayesianRidge(compute_score=True)\n",
    "clf_results = helpers.run_model(reg,DistanceToFirstStop_p80[0], DistanceToFirstStop_p80[1], DistanceToFirstStop_p80[2], DistanceToFirstStop_p80[3])\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "#reg = KernelRidge(alpha=1.0)\n",
    "#clf_results = helpers.run_model(reg,DistanceToFirstStop_p80[0], DistanceToFirstStop_p80[1], DistanceToFirstStop_p80[2], DistanceToFirstStop_p80[3])\n",
    "#clf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_time': '0:00:12.284308', 'run_score': 110.91236233221149}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "#score: 111.71\n",
    "clf = tree.DecisionTreeRegressor()\n",
    "clf_results = helpers.run_model(clf,DistanceToFirstStop_p80[0], DistanceToFirstStop_p80[1], DistanceToFirstStop_p80[2], DistanceToFirstStop_p80[3])\n",
    "clf_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#from sklearn.ensemble import StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'run_time': '0:00:43.873751', 'run_score': 86.83906446832533}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = RandomForestRegressor(n_estimators=10,min_samples_split=3, n_jobs=5)\n",
    "reg_results = helpers.run_model(reg,DistanceToFirstStop_p80[0], DistanceToFirstStop_p80[1], DistanceToFirstStop_p80[2], DistanceToFirstStop_p80[3])\n",
    "reg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': [1, 6, 11, 17, 22, 28, 33, 39, 44, 50], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n{'bootstrap': [True, False],\\n 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\\n 'max_features': ['auto', 'sqrt'],\\n 'min_samples_leaf': [1, 2, 4],\\n 'min_samples_split': [2, 5, 10],\\n 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\\n \""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 1, stop = 50, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\"\"\"\n",
    "{'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 28.8min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 106.9min\n",
      "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed: 193.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
       "                   estimator=RandomForestRegressor(bootstrap=True,\n",
       "                                                   criterion='mse',\n",
       "                                                   max_depth=None,\n",
       "                                                   max_features='auto',\n",
       "                                                   max_leaf_nodes=None,\n",
       "                                                   min_impurity_decrease=0.0,\n",
       "                                                   min_impurity_split=None,\n",
       "                                                   min_samples_leaf=1,\n",
       "                                                   min_samples_split=2,\n",
       "                                                   min_weight_fraction_leaf=0.0,\n",
       "                                                   n_estimators='warn',\n",
       "                                                   n_jobs=None, oob_score=False,\n",
       "                                                   random_sta...\n",
       "                   iid='warn', n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
       "                                                      70, 80, 90, 100, 110,\n",
       "                                                      None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [1, 6, 11, 17, 22, 28,\n",
       "                                                         33, 39, 44, 50]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=42, refit=True,\n",
       "                   return_train_score=False, scoring=None, verbose=2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(DistanceToFirstStop_p80[0], DistanceToFirstStop_p80[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 50,\n",
       " 'min_samples_split': 10,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 40,\n",
       " 'bootstrap': True}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_random.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "estimators = [('lr', RidgeCV()), ('svr', LinearSVR(random_state=42))]\n",
    "reg = StackingRegressor(\n",
    "    estimators=estimators,\n",
    "    final_estimator=RandomForestRegressor(n_estimators=10,\n",
    "                                          random_state=42)\n",
    "clf_results = helpers.run_model(reg,DistanceToFirstStop_p80[0], DistanceToFirstStop_p80[1], DistanceToFirstStop_p80[2], DistanceToFirstStop_p80[3])\n",
    "clf_results\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_20 = RandomForestRegressor(n_estimators=20,min_samples_split=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_20_results = []\n",
    "feature_importances = []\n",
    "for x in sets: \n",
    "    rf_20_results.append(helpers.run_model(rf_20,x[0],x[1],x[2],x[3]))\n",
    "    feature_importances.append(helpers.importance_df(rf_20, final_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_20_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.importance_df(rf_20, final_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models\n",
    "rf_20 = RandomForestRegressor(n_estimators=20,min_samples_split=3, n_jobs=-1)\n",
    "rf_30 = RandomForestRegressor(n_estimators=30,min_samples_split=3, n_jobs=-1)\n",
    "models = [rf_20, rf_30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_1=[]\n",
    "for i,model in enumerate(models): \n",
    "    runs.append({'run_'+str(i): helpers.run_model(model, X_train_1, X_validation_1, Y_train_1, Y_validation_1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_2=[]\n",
    "for i,model in enumerate(models): \n",
    "    runs.append({'run_'+str(i): helpers.run_model(model, X_train_2, X_validation_2, Y_train_2, Y_validation_2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "sv = svm.SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "etc = ExtraTreesRegressor(n_estimators=30,min_samples_split=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etc_results = helpers.run_model(etc,X_train_1, X_validation_1, Y_train_1, Y_validation_1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etc_results = RandomForestRegressor(n_estimators=40,min_samples_split=3, n_jobs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.run_model(rfg,X_train_1, X_validation_1, Y_train_1, Y_validation_1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random charts below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# list of x locations for plotting\n",
    "x_values = list(range(len(importances)))\n",
    "# Make a bar chart\n",
    "plt.bar(x_values, importances, orientation = 'vertical', color = 'r', edgecolor = 'k', linewidth = 1.2)\n",
    "# Tick labels for x axis\n",
    "plt.xticks(x_values, final_features, rotation='vertical')\n",
    "# Axis labels and title\n",
    "plt.ylabel('Importance'); plt.xlabel('Variable'); plt.title('Variable Importances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# List of features sorted from most to least important\n",
    "sorted_importances = [importance[1] for importance in feature_importances]\n",
    "sorted_features = [importance[0] for importance in feature_importances]\n",
    "# Cumulative importances\n",
    "cumulative_importances = np.cumsum(sorted_importances)\n",
    "# Make a line graph\n",
    "plt.plot(x_values, cumulative_importances, 'g-')\n",
    "# Draw line at 95% of importance retained\n",
    "plt.hlines(y = 0.95, xmin=0, xmax=len(sorted_importances), color = 'r', linestyles = 'dashed')\n",
    "# Format x ticks and labels\n",
    "plt.xticks(x_values, sorted_features, rotation = 'vertical')\n",
    "# Axis labels and title\n",
    "plt.xlabel('Variable'); plt.ylabel('Cumulative Importance'); plt.title('Cumulative Importances');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
