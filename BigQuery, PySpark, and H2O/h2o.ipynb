{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Engineering and Predictions</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Focus on feature selection, processing speed, model iteration</h3>\n",
    "<ul>\n",
    "    <li>Will create different training sets w/ reature mixes</li>\n",
    "    <li>PyTorch, Numba, Parralization, and Dask for processing speed</li>\n",
    "    <li>Processing tutorials: \n",
    "        <a href = 'https://towardsdatascience.com/speed-up-your-algorithms-part-3-parallelization-4d95c0888748'>speeding up your algorithms</a>, \n",
    "        <a href = 'https://towardsdatascience.com/improving-random-forest-in-python-part-1-893916666cd'>improving random forest</a>\n",
    "    </li>\n",
    "    <li>Split train into 3 train/test splits, run models and compare results before running on final test</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload #for changes in helpers\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import helpers\n",
    "reload(helpers)\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../geotab-data/train.csv')\n",
    "test = pd.read_csv('../geotab-data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Target PCA</h1>\n",
    "\n",
    "<ul>\n",
    "    <li>Light summary eda</li>\n",
    "    <li>Min max scaler and target PCA</li>\n",
    "    <li>Not sure if this is necessary for purpose of this notebook</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Summaries:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>dtypes</th>\n",
       "      <th>missing</th>\n",
       "      <th>unique</th>\n",
       "      <th>first_val</th>\n",
       "      <th>last_val</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>stdev</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>TotalTimeStopped_p20</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>273.0</td>\n",
       "      <td>1.731272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.080017</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>TotalTimeStopped_p50</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>7.681874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.553418</td>\n",
       "      <td>2.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>TotalTimeStopped_p80</td>\n",
       "      <td>int64</td>\n",
       "      <td>0</td>\n",
       "      <td>403</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>22.948071</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.118134</td>\n",
       "      <td>5.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>DistanceToFirstStop_p20</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>3479</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1902.7</td>\n",
       "      <td>6.564450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.003261</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>DistanceToFirstStop_p50</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>7483</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3099.5</td>\n",
       "      <td>28.255852</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.720090</td>\n",
       "      <td>4.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>DistanceToFirstStop_p80</td>\n",
       "      <td>float64</td>\n",
       "      <td>0</td>\n",
       "      <td>13267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4064.3</td>\n",
       "      <td>81.922639</td>\n",
       "      <td>60.4</td>\n",
       "      <td>152.682760</td>\n",
       "      <td>8.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name   dtypes  missing  unique  first_val  last_val     max       mean  median       stdev  entropy\n",
       "0     TotalTimeStopped_p20    int64        0     172        0.0       0.0   273.0   1.731272     0.0    7.080017     0.92\n",
       "1     TotalTimeStopped_p50    int64        0     264        0.0       0.0   343.0   7.681874     0.0   15.553418     2.70\n",
       "2     TotalTimeStopped_p80    int64        0     403        0.0       0.0   689.0  22.948071    16.0   28.118134     5.06\n",
       "3  DistanceToFirstStop_p20  float64        0    3479        0.0       0.0  1902.7   6.564450     0.0   28.003261     1.35\n",
       "4  DistanceToFirstStop_p50  float64        0    7483        0.0       0.0  3099.5  28.255852     0.0   71.720090     4.16\n",
       "5  DistanceToFirstStop_p80  float64        0   13267        0.0       0.0  4064.3  81.922639    60.4  152.682760     8.10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "targets = train.iloc[:,12:27]\n",
    "\n",
    "#targets to predict\n",
    "total_time = targets[['TotalTimeStopped_p20','TotalTimeStopped_p50', 'TotalTimeStopped_p80']]\n",
    "distance_to_first = targets[['DistanceToFirstStop_p20','DistanceToFirstStop_p50','DistanceToFirstStop_p80']]\n",
    "target_cols = list(total_time.columns) + list(distance_to_first.columns)\n",
    "p_targets = targets[target_cols]\n",
    "\n",
    "#optional targets\n",
    "time_from_first = targets[['TimeFromFirstStop_p20','TimeFromFirstStop_p50','TimeFromFirstStop_p80']]\n",
    "\n",
    "print('Target Summaries:')\n",
    "display(helpers.summarize(p_targets, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.66396904, 0.17536384, 0.07856878])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#scale targets w/ a min max scalers and append to train\n",
    "for col in target_cols:\n",
    "    train[col+str(\"_minmax\")] = (preprocessing.minmax_scale(train[col], feature_range=(0,1)))\n",
    "    \n",
    "min_max_cols = ['TotalTimeStopped_p20_minmax', 'TotalTimeStopped_p50_minmax',\n",
    "                'TotalTimeStopped_p80_minmax', 'DistanceToFirstStop_p20_minmax',\n",
    "                'DistanceToFirstStop_p50_minmax', 'DistanceToFirstStop_p80_minmax']\n",
    "\n",
    "pca = PCA(n_components=3, random_state=5)\n",
    "\n",
    "principalComponents = pca.fit_transform(train[min_max_cols])\n",
    "principalDf = pd.DataFrame(principalComponents)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Feature Engineering</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time and day features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making hour columns cyclical\n",
    "train = helpers.date_cyc_enc(train, 'Hour', 24)\n",
    "test = helpers.date_cyc_enc(test, 'Hour', 24) \n",
    "\n",
    "#encoding time of day\n",
    "train['is_day'] = train['Hour'].apply(lambda x: 1 if 7 < x < 18 else 0)\n",
    "test['is_day'] = test['Hour'].apply(lambda x: 1 if 7 < x < 18 else 0)\n",
    "\n",
    "train['is_morning'] = train['Hour'].apply(lambda x: 1 if 6 < x < 10 else 0)\n",
    "test['is_morning'] = test['Hour'].apply(lambda x: 1 if 6 < x < 10 else 0)\n",
    "\n",
    "train['is_night'] = train['Hour'].apply(lambda x: 1 if 17 < x < 20 else 0)\n",
    "test['is_night'] = test['Hour'].apply(lambda x: 1 if 17 < x < 20 else 0)\n",
    "\n",
    "#encoding weekend vs not weekend\n",
    "train['is_day_weekend'] = np.where((train['is_day'] == 1) & (train['Weekend'] == 1), 1,0)\n",
    "test['is_day_weekend'] = np.where((test['is_day'] == 1) & (train['Weekend'] == 1), 1,0)\n",
    "\n",
    "train['is_mor_weekend'] = np.where((train['is_morning'] == 1) & (train['Weekend'] == 1), 1,0)\n",
    "test['is_mor_weekend'] = np.where((test['is_morning'] == 1) & (train['Weekend'] == 1), 1,0)\n",
    "\n",
    "train['is_nig_weekend'] = np.where((train['is_night'] == 1) & (train['Weekend'] == 1), 1,0)\n",
    "test['is_nig_weekend'] = np.where((test['is_night'] == 1) & (train['Weekend'] == 1), 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Location and direction features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intersection and city concat\n",
    "train[\"Intersec\"] = train[\"IntersectionId\"].astype(str) + train[\"City\"]\n",
    "test[\"Intersec\"] = test[\"IntersectionId\"].astype(str) + test[\"City\"]\n",
    "\n",
    "#make numerical and drop concat\n",
    "le = LabelEncoder()\n",
    "le.fit(pd.concat([train[\"Intersec\"],test[\"Intersec\"]]).drop_duplicates().values)\n",
    "train[\"Intersec\"] = le.transform(train[\"Intersec\"])\n",
    "test[\"Intersec\"] = le.transform(test[\"Intersec\"])\n",
    "\n",
    "train['EntryType'] = train['EntryStreetName'].apply(helpers.road_encode)\n",
    "train['ExitType'] = train['ExitStreetName'].apply(helpers.road_encode)\n",
    "test['EntryType'] = test['EntryStreetName'].apply(helpers.road_encode)\n",
    "test['ExitType'] = test['ExitStreetName'].apply(helpers.road_encode)\n",
    "\n",
    "#map directional encoding in in train and test\n",
    "train['EntryHeading'] = train['EntryHeading'].map(helpers.directions)\n",
    "train['ExitHeading'] = train['ExitHeading'].map(helpers.directions)\n",
    "test['EntryHeading'] = test['EntryHeading'].map(helpers.directions)\n",
    "test['ExitHeading'] = test['ExitHeading'].map(helpers.directions)\n",
    "\n",
    "#heading differences will tell us the degree at which you traveled, (0 degrees is straight, 180 u turn)\n",
    "train['diffHeading'] = train['EntryHeading']-train['ExitHeading']  \n",
    "test['diffHeading'] = test['EntryHeading']-test['ExitHeading'] \n",
    "\n",
    "#if you stay on the same street feature\n",
    "train[\"same_str\"] = (train[\"EntryStreetName\"] ==  train[\"ExitStreetName\"]).astype(int)\n",
    "test[\"same_str\"] = (test[\"EntryStreetName\"] ==  test[\"ExitStreetName\"]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondary features, monthly rainfall by city "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the city and month into one variable\n",
    "train['city_month'] = train[\"City\"] + train[\"Month\"].astype(str)\n",
    "test['city_month'] = test[\"City\"] + test[\"Month\"].astype(str)\n",
    "\n",
    "# Creating a new column by mapping the city_month variable to it's corresponding average monthly rainfall\n",
    "train[\"average_rainfall\"] = train['city_month'].map(helpers.monthly_rainfall)\n",
    "test[\"average_rainfall\"] = test['city_month'].map(helpers.monthly_rainfall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace city w/ dummy variables, can't run cells above after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dummy variables for city, drops city\n",
    "try:\n",
    "    train = pd.get_dummies(train, columns=['City' ],prefix=['City'], drop_first=False)\n",
    "    test = pd.get_dummies(test, columns=['City' ],prefix=['City'], drop_first=False)\n",
    "except KeyError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale lat and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wonder if this would be different with min max\n",
    "scaler = preprocessing.StandardScaler()\n",
    "for col in ['Latitude','Longitude']:\n",
    "    scaler.fit(train[col].values.reshape(-1, 1))\n",
    "    train[col] = scaler.transform(train[col].values.reshape(-1, 1))\n",
    "    test[col] = scaler.transform(test[col].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['RowId', 'Path','EntryStreetName','ExitStreetName'],axis=1, inplace=True)\n",
    "test.drop(['RowId', 'Path','EntryStreetName','ExitStreetName'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = ['IntersectionId', 'Latitude', 'Longitude', 'EntryHeading',\n",
    "                    'ExitHeading', 'Hour', 'Weekend', 'Month',\n",
    "                    'is_morning', 'is_night', 'is_day_weekend', 'is_mor_weekend',\n",
    "                    'is_nig_weekend', \n",
    "                    #'Hour', \n",
    "                    'Hour_sin', \n",
    "                    'Hour_cos', \n",
    "                    'same_str', 'Intersec', 'EntryType',\n",
    "                    'ExitType', 'diffHeading', 'average_rainfall', 'is_day',\n",
    "                    'City_Boston', 'City_Chicago', 'City_Philadelphia', \n",
    "                    'City_Atlanta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Model building</h1>\n",
    "Useful Vaiables:\n",
    "<ul>\n",
    "    <li>final_features - list final set of features for prediction</li>\n",
    "    <li>target_cols - list of targets to predict</li>\n",
    "    <li>train - full train data set with derived features</li>\n",
    "    <li>test - full test data set with derived features</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (857409, 48)\n",
      "Test dataset shape:  (1920335, 27)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset shape: \"+ str(train.shape))\n",
    "print(\"Test dataset shape:  \"+ str(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Features: \n",
      " ['IntersectionId', 'Latitude', 'Longitude', 'EntryHeading', 'ExitHeading', 'Hour', 'Weekend', 'Month', 'is_morning', 'is_night', 'is_day_weekend', 'is_mor_weekend', 'is_nig_weekend', 'Hour_sin', 'Hour_cos', 'same_str', 'Intersec', 'EntryType', 'ExitType', 'diffHeading', 'average_rainfall', 'is_day', 'City_Boston', 'City_Chicago', 'City_Philadelphia', 'City_Atlanta'] \n",
      "\n",
      "Targets: \n",
      " ['TotalTimeStopped_p20', 'TotalTimeStopped_p50', 'TotalTimeStopped_p80', 'DistanceToFirstStop_p20', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p80']\n"
     ]
    }
   ],
   "source": [
    "#X and y for train set\n",
    "print('Final Features: \\n',final_features, '\\n\\nTargets: \\n', target_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Modeling w/ H2O</h2>\n",
    "<ul>\n",
    "    <li>train test split simple with H2O</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>5 hours 11 mins</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/Chicago</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.26.0.10</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 month and 4 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_devonnavon_2eb76m</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>6.791 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>12</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>12</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>{'http': None, 'https': None}</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.4 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ------------------------------------------------------------------\n",
       "H2O cluster uptime:         5 hours 11 mins\n",
       "H2O cluster timezone:       America/Chicago\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.26.0.10\n",
       "H2O cluster version age:    1 month and 4 days\n",
       "H2O cluster name:           H2O_from_python_devonnavon_2eb76m\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    6.791 Gb\n",
       "H2O cluster total cores:    12\n",
       "H2O cluster allowed cores:  12\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:       {'http': None, 'https': None}\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.7.4 final\n",
       "--------------------------  ------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h2o\n",
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Splitting Data</h2>\n",
    "<ul>\n",
    "    <li>og_train</li>\n",
    "    <li>og_test</li>\n",
    "    <li>final_features</li>\n",
    "    <li>target_cols</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "h2_train = h2o.H2OFrame(train)\n",
    "h2_test = h2o.H2OFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(643207, 48) (214202, 48)\n"
     ]
    }
   ],
   "source": [
    "og_split = h2_train.split_frame(ratios = [0.75], seed = 232)\n",
    "og_train = og_split[0] # using 80% for training\n",
    "og_test = og_split[1] #rest 20% for testing\n",
    "print(og_train.shape, og_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>H2O</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Linear Model (first try)</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.random_forest import H2ORandomForestEstimator\n",
    "rfe = H2ORandomForestEstimator(model_id=\"rf_covType_v1\",\n",
    "                                ntrees=120,\n",
    "                                #stopping_rounds=2,\n",
    "                                score_each_iteration=True,\n",
    "                                seed=12,\n",
    "                                nfolds=5,\n",
    "                                min_split_improvement=.0001\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "drf prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "rfe.train(x = final_features, y = 'TotalTimeStopped_p20', training_frame = h2_train, max_runtime_secs=0)\n",
    "pred1 = rfe.predict(h2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drf Model Build progress: |███████████████████████████████████████████████ (cancelled) 100%\n"
     ]
    },
    {
     "ename": "H2OJobCancelled",
     "evalue": "Job<$03017f00000132d4ffffffff$_ba3600ed135752151d63d23ee3874814> was cancelled by the user.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mH2OJobCancelled\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-040ab86da8dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'TotalTimeStopped_p50'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_runtime_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh2_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/estimators/estimator_base.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, max_runtime_secs, ignored_columns, model_id, verbose)\u001b[0m\n\u001b[1;32m    110\u001b[0m         self._train(x=x, y=y, training_frame=training_frame, offset_column=offset_column, fold_column=fold_column,\n\u001b[1;32m    111\u001b[0m                     \u001b[0mweights_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_column\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_frame\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_runtime_secs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_runtime_secs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     ignored_columns=ignored_columns, model_id=model_id, verbose=verbose)\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/estimators/estimator_base.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, x, y, training_frame, offset_column, fold_column, weights_column, validation_frame, max_runtime_secs, ignored_columns, model_id, verbose, extend_parms_fn)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_updates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_model_scoring_history\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mmodel_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GET /%d/Models/%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrest_ver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdest_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdest_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/h2o/job.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, poll_updates)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# check if failed... and politely print relevant message\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"CANCELLED\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mH2OJobCancelled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job<%s> was cancelled by the user.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"FAILED\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"stacktrace\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mH2OJobCancelled\u001b[0m: Job<$03017f00000132d4ffffffff$_ba3600ed135752151d63d23ee3874814> was cancelled by the user."
     ]
    }
   ],
   "source": [
    "rfe.train(x = final_features, y = 'TotalTimeStopped_p50', training_frame = h2_train, max_runtime_secs=0)\n",
    "pred2 = rfe.predict(h2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.train(x = final_features, y = 'TotalTimeStopped_p80', training_frame = h2_train, max_runtime_secs=0)\n",
    "pred3 = rfe.predict(h2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.train(x = final_features, y = 'DistanceToFirstStop_p20', training_frame = h2_train, max_runtime_secs=0)\n",
    "pred4 = rfe.predict(h2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.train(x = final_features, y = 'DistanceToFirstStop_p50', training_frame = h2_train, max_runtime_secs=0)\n",
    "pred5 = rfe.predict(h2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe.train(x = final_features, y = 'DistanceToFirstStop_p80', training_frame = h2_train, max_runtime_secs=0)\n",
    "pred6 = rfe.predict(h2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = h2o.as_list(pred1, use_pandas=True)\n",
    "pred2 = h2o.as_list(pred2, use_pandas=True)\n",
    "pred3 = h2o.as_list(pred3, use_pandas=True)\n",
    "pred4 = h2o.as_list(pred4, use_pandas=True)\n",
    "pred5 = h2o.as_list(pred5, use_pandas=True)\n",
    "pred6 = h2o.as_list(pred6, use_pandas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1.to_csv(\"../geotab-data/pred1.csv\")\n",
    "pred2.to_csv(\"../geotab-data/pred2.csv\")\n",
    "pred3.to_csv(\"../geotab-data/pred3.csv\")\n",
    "pred4.to_csv(\"../geotab-data/pred4.csv\")\n",
    "pred5.to_csv(\"../geotab-data/pred5.csv\")\n",
    "pred6.to_csv(\"../geotab-data/pred6.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,x in enumerate([pred1,pred2,pred3,pred4,pred5,pred6]):\n",
    "    x.columns=[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for i in range(len(pred1)):\n",
    "    for j in [np.array(pred1),np.array(pred2),np.array(pred3),np.array(pred4),np.array(pred5),np.array(pred6)]:\n",
    "        predictions.append(j[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../geotab-data/sample_submission.csv')\n",
    "submission['Target'] = predictions\n",
    "submission.to_csv(\"../geotab-data/h2opredictions.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
